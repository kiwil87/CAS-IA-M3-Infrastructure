{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9729ccd-7b06-4a41-bbe7-a671b10a1f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Pipeline using MLLib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75d1f88-8682-4584-b42f-08a91d1c40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Volumes/levkiwi_lakehouse/ml_sandbox/data/train.csv\"\n",
    "train_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Cast Boolean columns to int\n",
    "train_df = train_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Transported\", col(\"Transported\").cast(\"int\")) \n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0482d56-3631-409f-acb6-494b6f227346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Dataframes & MLLib pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d302f6b0-c7f7-4a0b-b93a-40d8b451b782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Step A: Define column lists\n",
    "numerical_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "categorical_cols = [\"HomePlanet\", \"Destination\", \"VIP\", \"CryoSleep\"]\n",
    "label_col = \"Transported\"  # The target\n",
    "\n",
    "# Step B: Imputer for numerical columns\n",
    "#         The Imputer in Spark can replace missing values with either mean or median (default is mean).\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCols=[col + \"_imputed\" for col in numerical_cols]\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "# Step C: For each categorical column, create a StringIndexer and OneHotEncoder\n",
    "#         We'll store these stages in a list to later build a Pipeline.\n",
    "stages = [imputer]\n",
    "\n",
    "indexer_output_cols = []\n",
    "ohe_output_cols = []\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    # Create the StringIndexer\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=cat_col,\n",
    "        outputCol=cat_col + \"_indexed\"\n",
    "    ).setHandleInvalid(\"keep\")  # you can decide how to handle unseen or null values\n",
    "    \n",
    "    # Create the OneHotEncoder\n",
    "    # For Spark 3.0+, OneHotEncoder can take multiple input and output columns, but here we do a single col for clarity\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=cat_col + \"_indexed\",\n",
    "        outputCol=cat_col + \"_ohe\"\n",
    "    )\n",
    "    \n",
    "    stages += [indexer, encoder]\n",
    "    indexer_output_cols.append(cat_col + \"_indexed\")\n",
    "    ohe_output_cols.append(cat_col + \"_ohe\")\n",
    "\n",
    "# Step D: Assemble all features (imputed numeric + OHE categorical) into a single \"features\" vector\n",
    "#         Weâ€™ll use the columns that were imputed for numeric, and the OHE output for categorical.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col + \"_imputed\" for col in numerical_cols] + ohe_output_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18378273-3878-4dfb-9c4f-35936f1d1c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build the Pipeline with all the stages\n",
    "preprocessing_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the Pipeline on train DataFrame\n",
    "preprocessing_pipeline = preprocessing_pipeline.fit(train_df)\n",
    "\n",
    "# Transform the train DataFrame\n",
    "train_transformed = preprocessing_pipeline.transform(train_df)\n",
    "\n",
    "# You can now use 'train_transformed' for any downstream tasks (model training, etc.)\n",
    "# 'train_transformed' will contain:\n",
    "# - The imputed columns: <colName>_imputed\n",
    "# - The indexed categorical columns: <colName>_indexed\n",
    "# - The one-hot encoded columns: <colName>_ohe\n",
    "# - A single assembled features vector column: \"features\"\n",
    "\n",
    "# Show a preview\n",
    "train_transformed.select(\n",
    "    [\"Age_imputed\", \"HomePlanet_ohe\", \"features\"]\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139464e2-21cb-4c7f-aae6-58d46ec5a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Decision Tree Classifier \n",
    "\n",
    "We extend the pipeline with a decision tree classifier to predict the Transported variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2885672-bbcd-4994-8870-5a066fd17d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Step E: StringIndexer for label column \"Transported\" -> \"label\"\n",
    "#         This transforms \"True\"/\"False\" (or any other categories) into numeric 0.0 or 1.0.\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=label_col,\n",
    "    outputCol=\"label\"\n",
    ").setHandleInvalid(\"keep\")\n",
    "stages += [label_indexer]\n",
    "\n",
    "# Define the hyperparameters for the DecisionTreeClassifier\n",
    "hyperparams = {\n",
    "    'impurity': 'entropy',          # Function to measure the quality of a split\n",
    "    'maxDepth': 3,                  # Limits the depth of the tree to prevent overfitting\n",
    "    'minInstancesPerNode': 10,      # The minimum number of samples required to be at a leaf node\n",
    "    'seed': 42                      # Ensures reproducibility of the results\n",
    "}\n",
    "\n",
    "# Step F: DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    **hyperparams\n",
    ")\n",
    "\n",
    "stages += [dt]\n",
    "\n",
    "# Build the Pipeline\n",
    "model_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the Pipeline on train_df\n",
    "model_pipeline = model_pipeline.fit(train_df)\n",
    "\n",
    "# Transform the DataFrame\n",
    "train_transformed = model_pipeline.transform(train_df)\n",
    "\n",
    "train_transformed.select(\n",
    "    \"label\", \"features\", \"prediction\", \"probability\"\n",
    ").show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ML_Pipeline_spark_ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
