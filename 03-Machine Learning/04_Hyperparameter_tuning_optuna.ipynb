{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf89f6c4-d472-4057-bd0a-61c58754b249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Pipeline using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d680960-9bf6-4f64-be2d-793a5d99e0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG levkiwi_lakehouse\")\n",
    "spark.sql(\"USE SCHEMA ml_sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75d1f88-8682-4584-b42f-08a91d1c40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Volumes/levkiwi_lakehouse/ml_sandbox/data/train.csv\"\n",
    "train_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Cast Boolean columns to int\n",
    "train_df = train_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Transported\", col(\"Transported\").cast(\"int\")) \n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0482d56-3631-409f-acb6-494b6f227346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandas & scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27ed428-55cb-46ef-b915-c522eb99334d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = train_df.toPandas()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d302f6b0-c7f7-4a0b-b93a-40d8b451b782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Define transformers for different column types\n",
    "numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    ")\n",
    "\n",
    "categorical_cols = ['HomePlanet', 'Destination', 'VIP', 'CryoSleep']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 2: Create a ColumnTransformer that applies the transformations to the columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Step 3: Assemble the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(train)\n",
    "\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe53945-d4f5-4c3a-8e0d-f4d194869005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting back to Pandas DataFrame\n",
    "onehot_encoder_feature_names = list(preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out())\n",
    "column_order =  numerical_cols + onehot_encoder_feature_names\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=column_order, index=train.index)\n",
    "y = train['Transported']\n",
    "\n",
    "X_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139464e2-21cb-4c7f-aae6-58d46ec5a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter tuning of a Decision Tree Classifier \n",
    "\n",
    "We use optuna to hyperparameter tuning of a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6945428-2cb3-4ce4-9c5e-5dc1d46f4b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', random_state= 42)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        # trial parameters to optimize\n",
    "        'max_depth' : trial.suggest_int('max_depth', 3, 40, log=True),\n",
    "        'min_samples_split' : trial.suggest_float('min_samples_split', 1e-6, 1e-3, log=True),\n",
    "        'min_samples_leaf' : trial.suggest_float('min_samples_leaf', 1e-6, 1e-3, log=True)\n",
    "    }\n",
    "\n",
    "    model.set_params(**params)\n",
    "\n",
    "    cv_score = cross_val_score(model, X_preprocessed, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    return cv_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"best_params =\", study.best_params, \"with cross_validation_score =\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f74f4a-3e61-4392-8fb3-cea4a34305cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c864b400-ab60-4c57-b252-e0932e89fc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set the tracking URI to the Databricks workspace\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Create an instance of MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "X = train.drop(['Transported'], axis = 1)\n",
    "y = train['Transported']\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Fit the model with the best hyperparameters from the study\n",
    "    model = DecisionTreeClassifier(criterion= 'entropy', random_state= 42)\n",
    "    model.set_params(**study.best_params)\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "    model_pipeline.fit(X, y)\n",
    "    \n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(study.best_params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", study.best_value)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Simple Decision Tree Classifier\")\n",
    "    \n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X, model_pipeline.predict(X))\n",
    "    \n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model_pipeline,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"decision_tree_model\",\n",
    "        artifact_path=\"decision_tree_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c94f634-cffa-472a-824d-fe0649c5957f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Hyperparameter_tuning_optuna",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
